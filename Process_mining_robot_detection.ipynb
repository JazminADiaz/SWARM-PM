{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello there, welcome to the application of process mining for analying robot swarms behaviours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import csv\n",
    "\n",
    "\n",
    "import shutil\n",
    "from IPython.utils.io import capture_output\n",
    "import random\n",
    "import re\n",
    "\n",
    "#change this to your path.\n",
    "dir_tutti=\"/home/jazmin/argos3-installation/tuttifrutti\"\n",
    "argos=\"tuttiTamT_A_r\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each mision consist on a set of activities, this script works for one mision, but with probability of sucess of: 1, 0.75, 0.5 and 0.25.\n",
    "\n",
    "First we create a folder for the whole mision, you'll have the \"activies\" file, where you can find out what mision was being executed.\n",
    "\n",
    "You will also find 4 folders, where you can find some results for each probability, such as, models, token replay fitness, and metrics for model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, we'll create a folder for this mision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where you want to search for folders\n",
    "directory = dir_tutti+\"/Logs\"\n",
    "\n",
    "# Prefix that should be in the folder names\n",
    "prefix = \"example\"\n",
    "\n",
    "# Find all folders with the \"example\" prefix in the name\n",
    "folders = [name for name in os.listdir(directory) if prefix in name]\n",
    "\n",
    "# Find the highest number in the folder names\n",
    "numbers = [int(name.replace(prefix, \"\")) for name in folders]\n",
    "highest_number = max(numbers) if numbers else 0\n",
    "\n",
    "# Create the name for the new folder\n",
    "new_name = f\"{prefix}{highest_number + 1}\"\n",
    "\n",
    "# Full path of the new folder\n",
    "new_folder = os.path.join(directory, new_name)\n",
    "\n",
    "#Lets created.\n",
    "if not os.path.exists(new_folder):\n",
    "    os.mkdir(new_folder)\n",
    "#inside the folder we need the files, \"EventLogs\" and \"Eventlogs_fail, let's created them\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we add the activities of the mision.\n",
    "\n",
    "Go ahead and select the option for a random collection of activities or use manual and instroduce it yourself. (For the manual option there's already an example you can modify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the word \"manual\" or \"random\"\n",
    "mision_activities=\"manual\"\n",
    "\n",
    "#number of times the mision is repeated to be stored in one log\n",
    "repetition_numer=2\n",
    "\n",
    "\n",
    "#duration of the simulation in seconds\n",
    "timer=1000\n",
    "\n",
    "actions=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy folder fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_folder(source, destination):\n",
    "    try:\n",
    "        # Create the new folder if it doesn't exist\n",
    "        if not os.path.exists(destination):\n",
    "            os.mkdir(destination)\n",
    "        # Use the `copytree` function from shutil to copy the folder and its contents\n",
    "        shutil.copytree(source, os.path.join(destination, os.path.basename(source)))\n",
    "        print(f\"Folder '{os.path.basename(source)}' copied to {destination}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to copy the folder: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file(source, destination):\n",
    "    try:\n",
    "        # Use the `copy` function from shutil to copy the file\n",
    "        shutil.copy(source, destination)\n",
    "        print(f\"File copied from {source} to {destination}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to copy the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activities function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activities():\n",
    "    if mision_activities==\"manual\":   \n",
    "        ################################ ADD THE ACTIVITIES #######################################################     #delete previos content\n",
    "        file=dir_tutti+\"/activities/activities.txt\"\n",
    "        with open(file, \"w\") as f:\n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"activities[\\\"0_sec\\\"] = {1,9,2};\")\n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"activities[\\\"1_con\\\"] = {0,6,7,3,5,1};\")\n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"activities[\\\"2_sec\\\"] = {11,8,1};\")\n",
    "\n",
    "        file=dir_tutti+\"/activities/activities.txt\"\n",
    "        with open(file, \"w\") as f:\n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"activities[\\\"0_sec\\\"] = {1,9,2};\")\n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"activities[\\\"1_con\\\"] = {0,6,7,3,5,1};\")\n",
    "            f.write(\"\\n\") \n",
    "            f.write(\"activities[\\\"2_sec\\\"] = {11,8,1};\")\n",
    "\n",
    "\n",
    "\n",
    "            ################################ ADD THE ACTIVITIES #######################################################\n",
    "\n",
    "    if mision_activities==\"random\":\n",
    "        #max_total_values = random.choices(numbers_total, weights_t)[0]\n",
    "        # Número máximo total entre todas las listas\n",
    "        max_total_values = actions  # Cambiar según sea necesario\n",
    "        print(f\"max: {max_total_values}\")\n",
    "\n",
    "        sum_total = 0\n",
    "        dictionary_lists = {}\n",
    "\n",
    "        list_counter = 0\n",
    "        initial_word = random.choice([\"con\\\"\", \"sec\\\"\"])\n",
    "        previous_word = initial_word\n",
    "\n",
    "        while sum_total != max_total_values:\n",
    "            # Genera una lista de números sin repetición\n",
    "            minimo = min(6, max_total_values - sum_total)\n",
    "            if minimo == 1:\n",
    "                break\n",
    "            numbers_random = list(range(2, min(6, max_total_values - sum_total) + 1))\n",
    "            # Asigna el mismo peso a cada número (en este caso, 1)\n",
    "            weights_n = [1] * len(numbers_random)\n",
    "\n",
    "            # Elige un número aleatorio con igual probabilidad\n",
    "            random_number = random.choices(numbers_random, weights_n)[0]\n",
    "\n",
    "            if sum_total + random_number <= max_total_values:\n",
    "                sum_total += random_number\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if previous_word == \"con\\\"\":\n",
    "                current_word = \"sec\\\"\"\n",
    "            else:\n",
    "                current_word = \"con\\\"\"\n",
    "\n",
    "            key = f\"\\\"{list_counter}_{current_word}\"\n",
    "\n",
    "            # Genera números aleatorios con igual probabilidad utilizando random.choices\n",
    "            if current_word == \"con\\\"\":\n",
    "                # Crear población excluyendo valores ya seleccionados en \"con\\\"\"\n",
    "                used_values = set(dictionary_lists.get(f\"\\\"{list_counter}_con\\\"\", []))\n",
    "                population = set(range(2, 12)) - used_values\n",
    "                random_numbers = random.sample(population, random_number)\n",
    "            else:\n",
    "                random_numbers = random.choices(range(2, 12), k=random_number)\n",
    "\n",
    "            dictionary_lists[key] = random_numbers\n",
    "            list_counter += 1\n",
    "            previous_word = current_word\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Total sum:\", sum_total)\n",
    "        print(\"Dictionary of lists:\")\n",
    "\n",
    "        file=dir_tutti+\"/activities/activities.txt\"\n",
    "        with open(file, \"w\") as f:        \n",
    "            for key, lista in dictionary_lists.items():\n",
    "                print(f'{key}: {lista}')  \n",
    "                lista_str = ', '.join(map(str, lista))\n",
    "                f.write(\"\\n\") \n",
    "                f.write(\"\\n\") \n",
    "                f.write(\"\\n\") \n",
    "                f.write(\"\\n\") \n",
    "                f.write(\"activities[\" + key + \"] = {\" + lista_str + \"};\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "def simulacion(directorio, repetition, argos, dir_tutti):\n",
    "    filelist = glob.glob(os.path.join(directorio+\"/EventLogs/\", \"*\"))\n",
    "    for f in filelist:\n",
    "        os.remove(f)\n",
    "\n",
    "    for be in range(0, repetition ):\n",
    "        \n",
    "        with open(f\"{dir_tutti}/experiments-loop-functions/scenarios/tuttifrutti/{argos}.argos\") as fp:\n",
    "            soup = bs(fp, 'html.parser')\n",
    "        \n",
    "        tag = soup.experiment\n",
    "        tag['random_seed'] = be\n",
    "\n",
    "        with open(f\"{dir_tutti}/experiments-loop-functions/scenarios/tuttifrutti/{argos}.argos\", \"w\") as outf:\n",
    "            outf.write(str(soup))\n",
    "        os.system(f\"cd {dir_tutti}/experiments-loop-functions\")\n",
    "\n",
    "        os.chdir(f'{dir_tutti}/experiments-loop-functions/build')\n",
    "\n",
    "        os.system('make')\n",
    "\n",
    "        os.chdir(f'{dir_tutti}/experiments-loop-functions/scenarios/tuttifrutti/')\n",
    "        os.system(f\"argos3 -c {argos}.argos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a folder for each posibility of success, and launching the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_activities=dir_tutti+\"/activities/activities.txt\"\n",
    "probabilities = [0.25,0.5,0.75,1]\n",
    "prueba=0.0\n",
    "activities()\n",
    "copy_file(file_activities,new_folder )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for probability in probabilities:\n",
    "    path_prob=f\"{new_folder}/prob_{probability}\"\n",
    "\n",
    "    file_path=f\"{new_folder}/prob_{probability}/EventLogs/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path, exist_ok=True)\n",
    "    file_path2=f\"{new_folder}/prob_{probability}/Big_log/\"\n",
    "    if not os.path.exists(file_path2):\n",
    "        os.makedirs(file_path2, exist_ok=True)\n",
    "\n",
    "\n",
    "for probability in probabilities:\n",
    "    with open(f\"{dir_tutti}/activities/activities.txt\", \"r\") as archivo:\n",
    "        lineas = archivo.readlines()\n",
    "    lineas[0] = f\"probability={probability};\"+\"\\n\"\n",
    "    lineas[1] = f\"events_folder=\\\"{new_folder}/prob_{probability}\\\";\"+\"\\n\"\n",
    "    lineas[2] = f\"timer_simulation={timer};\"+\"\\n\"\n",
    "\n",
    "    with open(f\"{dir_tutti}/activities/activities.txt\", \"w\") as archivo:\n",
    "        archivo.writelines(lineas)\n",
    "    simulacion(path_prob, repetition_numer, \"tuttiTamT_A_r\", dir_tutti)\n",
    "    \n",
    "path_prob=f\"{new_folder}/prob_p_{prueba}\"\n",
    "file_path=f\"{new_folder}/prob_p_{prueba}/EventLogs/\"\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path, exist_ok=True)\n",
    "file_path2=f\"{new_folder}/prob_p_{prueba}/Big_log/\"\n",
    "if not os.path.exists(file_path2):\n",
    "    os.makedirs(file_path2, exist_ok=True)\n",
    "\n",
    "with open(f\"{dir_tutti}/activities/activities.txt\", \"r\") as archivo:\n",
    "    lineas = archivo.readlines()\n",
    "lineas[0] = f\"probability={prueba};\"+\"\\n\"\n",
    "lineas[1] = f\"events_folder=\\\"{new_folder}/prob_p_{prueba}\\\";\"+\"\\n\"\n",
    "lineas[2] = f\"timer_simulation={timer};\"+\"\\n\"\n",
    "\n",
    "with open(f\"{dir_tutti}/activities/activities.txt\", \"w\") as archivo:\n",
    "    archivo.writelines(lineas)\n",
    "simulacion(path_prob, repetition_numer, \"tuttiTamT_A\", dir_tutti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pm4py libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.objects.petri_net.utils.decomposition import decompose\n",
    "from pm4py.objects.petri_net.utils import reachability_graph\n",
    "from pm4py.visualization.transition_system import visualizer as ts_visualizer\n",
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay\n",
    "from pm4py.objects.log.util import interval_lifecycle\n",
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to concatenate all event logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_name(name, prefix):\n",
    "    parts = re.match(fr'{re.escape(prefix)}seed_([0-9]+)\\.csv', name)\n",
    "    if parts:\n",
    "        return int(parts.group(1))\n",
    "    else:\n",
    "        return -1  # Otra forma de manejar nombres no válidos\n",
    "\n",
    "\n",
    "def concatenate(folder):\n",
    "    print((f\"{folder}/EventLogs/*.csv\"))\n",
    "    filenames = glob.glob(f\"{folder}/EventLogs/*.csv\")\n",
    "    filenames = sorted(filenames, key=lambda x: split_name(x, prefix=f\"{folder}/EventLogs/\"))\n",
    "    dfs = []\n",
    "    model=0\n",
    "    for filename in filenames:\n",
    "        log = pd.read_csv(filename, sep=',')\n",
    "        case_id=[]\n",
    "        case_id += len(log.mision) * [\"mision \"+str(model)]\n",
    "        log['mision']=case_id\n",
    "        model+=1\n",
    "        dfs.append(log)\n",
    "        big_frame = pd.concat(dfs, ignore_index=True)\n",
    "        #if model==len(filenames):\n",
    "        big_frame.to_csv(f\"{folder}/Big_log/final_log_2.csv\", sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create de dataframe for each posibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for probability in probabilities:\n",
    "    file_path=f\"{new_folder}/prob_{probability}\"\n",
    "    concatenate(file_path)\n",
    "    events = pd.read_csv(f\"{file_path}/Big_log/final_log_2.csv\")\n",
    "    events.columns = ['mision', 'action', 'datetime', 'resource']\n",
    "    events['datetime'] = pd.to_datetime(events['datetime'])\n",
    "    if probability==0.25:\n",
    "        events_low=events\n",
    "    if probability==0.5:\n",
    "        events_medium=events\n",
    "    if probability==0.75:\n",
    "        events_high=events\n",
    "    if probability==1:\n",
    "        events_ideal=events\n",
    "        \n",
    "file_path=f\"{new_folder}/prob_p_{prueba}\"\n",
    "concatenate(file_path)\n",
    "events = pd.read_csv(f\"{file_path}/Big_log/final_log_2.csv\")\n",
    "events.columns = ['mision', 'action', 'datetime', 'resource']\n",
    "events['datetime'] = pd.to_datetime(events['datetime'])\n",
    "events_prueba=events\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(events_ideal)\n",
    "\n",
    "# Obtener el encabezado (head) del DataFrame\n",
    "df_head = df.head()\n",
    "\n",
    "# Crear una figura y un eje para la tabla\n",
    "fig, ax = plt.subplots(figsize=(6, 2))  # Ajusta el tamaño según tus preferencias\n",
    "\n",
    "# Crear la tabla\n",
    "tabla = plt.table(cellText=df_head.values, colLabels=df_head.columns, loc='center')\n",
    "\n",
    "# Configurar el aspecto de la tabla\n",
    "tabla.auto_set_font_size(False)\n",
    "tabla.set_fontsize(10)\n",
    "tabla.scale(2, 2)  # Ajusta el tamaño de la tabla\n",
    "\n",
    "# Ocultar los ejes\n",
    "ax.axis('off')\n",
    "\n",
    "# Guardar la figura como imagen PNG\n",
    "plt.savefig(f'{new_folder}/tabla_head.png', bbox_inches='tight', pad_inches=0.1, dpi=300)  # Ajusta el nombre del archivo y los ajustes según tus preferencias\n",
    "\n",
    "# Mostrar la tabla en la pantalla (opcional)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting dataframes to eventlogs accepted by pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_ideal.rename(columns={'datetime': 'time:timestamp', 'mision': 'case:concept:name', 'action': 'concept:name', 'resource': 'org:resource'}, inplace=True)\n",
    "events_high.rename(columns={'datetime': 'time:timestamp', 'mision': 'case:concept:name', 'action': 'concept:name', 'resource': 'org:resource'}, inplace=True)\n",
    "events_medium.rename(columns={'datetime': 'time:timestamp', 'mision': 'case:concept:name', 'action': 'concept:name', 'resource': 'org:resource'}, inplace=True)\n",
    "events_low.rename(columns={'datetime': 'time:timestamp', 'mision': 'case:concept:name', 'action': 'concept:name', 'resource': 'org:resource'}, inplace=True)\n",
    "events_prueba.rename(columns={'datetime': 'time:timestamp', 'mision': 'case:concept:name', 'action': 'concept:name', 'resource': 'org:resource'}, inplace=True)\n",
    "log_pi = log_converter.apply(events_ideal)\n",
    "log_pv= log_converter.apply(events_ideal)\n",
    "log_ph = log_converter.apply(events_high)\n",
    "log_pm = log_converter.apply(events_medium)\n",
    "log_pl = log_converter.apply(events_low)\n",
    "log_prueba=log_converter.apply(events_prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure our log with probsbility 1 is ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check for variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = pm4py.get_variants(log_pi)\n",
    "len(variants)\n",
    "#variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter the log to the most usual variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "log_variants = pm4py.filter_variants_top_k(log_pi, k)\n",
    "#log_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this log is suppose to be ideal, we takeout any execution of the mision that ended up in time-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pi = pm4py.filter_event_attribute_values(log_pi, \"concept:name\", [\"time_out\"], level=\"case\", retain=False)\n",
    "\n",
    "for i in range(13):\n",
    "    log_pi = pm4py.filter_event_attribute_values(log_pi, \"concept:name\", [f\"T_{i}_rebooting\"], level=\"case\", retain=False)\n",
    "\n",
    "len(log_pi)\n",
    "#log_no_timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the model using different algorithims, in this case since the random option can reach high levels of concurrencie we'll work with heuristic and inductive miner, alpha is also an option you can comment out if the mision is not high on concurrencie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha():\n",
    "    for probability in probabilities:\n",
    "        file_path=f\"{new_folder}/prob_{probability}\"\n",
    "        if probability==0.25:\n",
    "            net, im, fm = pm4py.discover_petri_net_alpha(log_pl)\n",
    "            parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "            gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pl)\n",
    "            pn_visualizer.save(gviz, f\"{file_path}/Alpha_miner_{probability}.png\")\n",
    "            #pm4py.view_petri_net(net, im, fm)\n",
    "        if probability==0.5:\n",
    "            net, im, fm = pm4py.discover_petri_net_alpha(log_pm)\n",
    "            parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "            gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pm)\n",
    "            pn_visualizer.save(gviz, f\"{file_path}/Alpha_miner_{probability}.png\")\n",
    "            #pm4py.view_petri_net(net, im, fm)\n",
    "        if probability==0.75:\n",
    "            net, im, fm = pm4py.discover_petri_net_alpha(log_ph)\n",
    "            parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "            gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_ph)\n",
    "            pn_visualizer.save(gviz, f\"{file_path}/Alpha_miner_{probability}.png\")\n",
    "            #pm4py.view_petri_net(net, im, fm)\n",
    "        if probability==1:\n",
    "            net, im, fm = pm4py.discover_petri_net_alpha(log_pi)\n",
    "            parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "            gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pi)\n",
    "            pn_visualizer.save(gviz, f\"{file_path}/Alpha_miner_{probability}.png\")\n",
    "            #pm4py.view_petri_net(net, im, fm)\n",
    "\n",
    "#alpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for probability in probabilities:\n",
    "    file_path=f\"{new_folder}/prob_{probability}\"\n",
    "    if probability==0.25:\n",
    "        net, im, fm = pm4py.discover_petri_net_heuristics(log_pl, dependency_threshold=0.99)\n",
    "        parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "        gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pl)\n",
    "        pn_visualizer.save(gviz, f\"{file_path}/Heuristic_miner_{probability}.png\")\n",
    "        #pm4py.view_petri_net(net, im, fm)\n",
    "    if probability==0.5:\n",
    "        net, im, fm = pm4py.discover_petri_net_heuristics(log_pm, dependency_threshold=0.99)\n",
    "        parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "        gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pm)\n",
    "        pn_visualizer.save(gviz, f\"{file_path}/Heuristic_miner_{probability}.png\")\n",
    "        #pm4py.view_petri_net(net, im, fm)\n",
    "    if probability==0.75:\n",
    "        net, im, fm = pm4py.discover_petri_net_heuristics(log_ph, dependency_threshold=0.99)\n",
    "        parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "        gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_ph)\n",
    "        pn_visualizer.save(gviz, f\"{file_path}/Heuristic_miner_{probability}.png\")\n",
    "        #pm4py.view_petri_net(net, im, fm)\n",
    "    if probability==1:\n",
    "        net, im, fm = pm4py.discover_petri_net_heuristics(log_pi, dependency_threshold=0.99)\n",
    "        parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "        gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pi)\n",
    "        pn_visualizer.save(gviz, f\"{file_path}/Heuristic_miner_{probability}.png\")\n",
    "        #pm4py.view_petri_net(net, im, fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inductivo miner\n",
    "\n",
    "We'll use inductive miner at the end beause we'll use it for conformance checking and like this variables as net, initial_marking, final_marking are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for probability in probabilities:\n",
    "    file_path=f\"{new_folder}/prob_{probability}\"\n",
    "    if probability==0.25:\n",
    "        net, im, fm = pm4py.discover_petri_net_inductive(log_pl)\n",
    "        parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "        gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pl)\n",
    "        pn_visualizer.save(gviz, f\"{file_path}/Inductive_miner_{probability}.png\")\n",
    "        #pm4py.view_petri_net(net, im, fm)\n",
    "    if probability==0.5:\n",
    "        net, im, fm = pm4py.discover_petri_net_inductive(log_pm)\n",
    "        parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "        gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pm)\n",
    "        pn_visualizer.save(gviz, f\"{file_path}/Inductive_miner_{probability}.png\")\n",
    "        #pm4py.view_petri_net(net, im, fm)\n",
    "    if probability==0.75:\n",
    "        net, im, fm = pm4py.discover_petri_net_inductive(log_ph)\n",
    "        parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "        gviz = pn_visualizer.apply(net, im, fm , parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_ph)\n",
    "        pn_visualizer.save(gviz, f\"{file_path}/Inductive_miner_{probability}.png\")\n",
    "        #pm4py.view_petri_net(net, im, fm)\n",
    "    if probability==1:\n",
    "        net_comp, im_comp, fm_comp = pm4py.discover_petri_net_inductive(log_pi)\n",
    "        parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "        gviz = pn_visualizer.apply(net_comp, im_comp, fm_comp, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_pi)\n",
    "        pn_visualizer.save(gviz, f\"{file_path}/Inductive_miner_{probability}.png\")\n",
    "        #pm4py.view_petri_net(net, im, fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the posibility of sucess bellow 1 we intend to represent anomalies that may happen in the execution of a mision, therefore, we present someways to discover this anomalies, it will be necessary to adapt the parameters to your needs. \n",
    "\n",
    "You can alternate between log_pi, log_ph, log_pm, log_pl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if all cases happend within a normal range of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_performance = pm4py.filter_case_performance(log_ph, 0, 100)\n",
    "#log_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the variants, activities and resources. \n",
    "\n",
    "\n",
    "Note: the resource can be exchange between robot and random_seed, to do so go back to the log converter and change the name of the resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = pm4py.get_variants(log_pl)\n",
    "activities = pm4py.get_event_attribute_values(log_ph, \"concept:name\")\n",
    "resources = pm4py.get_event_attribute_values(log_pl, \"org:resource\")\n",
    "len(variants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the probability of failure is too high you may enconter filtering the log to only the most k used variants useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "log_variantes = pm4py.filter_variants_top_k(log_pl, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the log by atribute, at event level and case level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can get all the events containing a attribute\n",
    "tracefilter_log_event = pm4py.filter_event_attribute_values(log_pm, \"org:resource\", [6], level=\"event\", retain=False)\n",
    "#Or all the cases containing a attribute\n",
    "#tracefilter_log_case = pm4py.filter_event_attribute_values(log_pm, \"org:resource\", [7], level=\"case\", retain=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for cases were two activities are follow by one anohter.\n",
    "example: how many times an activity rebooted when x activity happend first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_between = pm4py.filter_between(log_pl, \"T_49_Busy\", \"T_49_rebooting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To eliminate noise, we can eliminte logs that are too long (rework) or too short (incompleate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_by_size = pm4py.filter_case_size(log_pm, 0,10 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for activities happening more than k times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "filtered_log = pm4py.filter_activities_rework(log_pl, 'simulation_successful', k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great tool that allows us to check exactly activities that follow one another are directly follows diagrams, we have some bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for probability in probabilities:\n",
    "    file_path=f\"{new_folder}/prob_{probability}\"\n",
    "    if probability==0.25:\n",
    "        performance_dfg, start_activities, end_activities = pm4py.discover_performance_dfg(log_pl)\n",
    "        pm4py.save_vis_performance_dfg(performance_dfg, start_activities, end_activities, f'{file_path}/dfg_{probability}.svg')\n",
    "    if probability==0.5:\n",
    "        performance_dfg, start_activities, end_activities = pm4py.discover_performance_dfg(log_pm)\n",
    "        pm4py.save_vis_performance_dfg(performance_dfg, start_activities, end_activities, f'{file_path}/dfg_{probability}.svg')\n",
    "    if probability==0.75:\n",
    "        performance_dfg, start_activities, end_activities = pm4py.discover_performance_dfg(log_ph)\n",
    "        pm4py.save_vis_performance_dfg(performance_dfg, start_activities, end_activities, f'{file_path}/dfg_{probability}.svg')\n",
    "    if probability==1:\n",
    "        performance_dfg, start_activities, end_activities = pm4py.discover_performance_dfg(log_pi)\n",
    "        pm4py.save_vis_performance_dfg(performance_dfg, start_activities, end_activities, f'{file_path}/dfg_{probability}.svg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is too complicated, but you want to analize it visually you can decompose it into smaller pieces, here you have the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_f, initial_marking_f, final_marking_f = pm4py.discover_petri_net_inductive(log_pl)\n",
    "list_nets = decompose(net_f, initial_marking_f, final_marking_f)\n",
    "for index, model in enumerate(list_nets):\n",
    "    subnet, s_im, s_fm = model\n",
    "    #pm4py.view_petri_net(subnet, s_im, s_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformance Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token-based replay\n",
    "\n",
    " Based on this comparison, token replay provides insights into whether the recorded process adheres to the expected process model. It can identify deviations, missing steps, or extra steps in the actual process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traces(log,folder_prob):\n",
    "    # Create a list to store trace results\n",
    "    trace_results_list = []\n",
    "\n",
    "    replayed_traces = pm4py.conformance_diagnostics_token_based_replay(log, net_comp, im_comp, fm_comp)\n",
    "\n",
    "    # Iterate through the replayed traces and access conformance metrics for each trace\n",
    "    for i, trace_result in enumerate(replayed_traces, start=1):\n",
    "        trace_is_fit = trace_result['trace_is_fit']  # Boolean indicating if the trace fits the model\n",
    "        trace_fitness = trace_result['trace_fitness']  # Fitness value for the trace\n",
    "        missing_tokens = trace_result['missing_tokens']  # Number of missing tokens\n",
    "        consumed_tokens = trace_result['consumed_tokens']  # Number of consumed tokens\n",
    "        remaining_tokens = trace_result['remaining_tokens']  # Number of remaining tokens\n",
    "        produced_tokens = trace_result['produced_tokens']  # Number of produced tokens\n",
    "\n",
    "        # Create a dictionary to store trace results\n",
    "        trace_result_dict = {\n",
    "            \"Trace Number\": i,\n",
    "            \"Trace Is Fit\": trace_is_fit,\n",
    "            \"Trace Fitness\": trace_fitness,\n",
    "            \"Missing Tokens\": missing_tokens,\n",
    "            \"Consumed Tokens\": consumed_tokens,\n",
    "            \"Remaining Tokens\": remaining_tokens,\n",
    "            \"Produced Tokens\": produced_tokens\n",
    "        }\n",
    "\n",
    "        # Append the trace result dictionary to the list\n",
    "        trace_results_list.append(trace_result_dict)\n",
    "\n",
    "        # Access other information as needed\n",
    "        activated_transitions = trace_result['activated_transitions']\n",
    "        reached_marking = trace_result['reached_marking']\n",
    "        transitions_with_problems = trace_result['transitions_with_problems']\n",
    "        enabled_transitions_in_marking = trace_result['enabled_transitions_in_marking']\n",
    "    # Define the path to the CSV file\n",
    "    csv_file_path = folder_prob+\"/trace_results.csv\"\n",
    "\n",
    "    # Define the CSV fieldnames\n",
    "    fieldnames = [\"Trace Number\", \"Trace Is Fit\", \"Trace Fitness\", \"Missing Tokens\", \"Consumed Tokens\", \"Remaining Tokens\", \"Produced Tokens\"]\n",
    "\n",
    "    # Write the trace results to a CSV file with separators\n",
    "    \n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header row\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for trace_result_dict in trace_results_list:\n",
    "            # Write the trace results\n",
    "            writer.writerow(trace_result_dict)\n",
    "            \n",
    "            # Write a separator row\n",
    "            writer.writerow({key: '-' * 10 for key in fieldnames})\n",
    "    \n",
    "    print(f\"Trace results have been exported to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.conformance.tokenreplay.diagnostics import duration_diagnostics\n",
    "\n",
    "for probability in probabilities:\n",
    "    file_path=f\"{new_folder}/prob_{probability}\"\n",
    "    if probability==0.25:\n",
    "        traces(log_pl,file_path )\n",
    "    if probability==0.5:\n",
    "        traces(log_pm,file_path )\n",
    "    if probability==0.75:\n",
    "        traces(log_ph,file_path )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnostics (TBR)\n",
    "\n",
    "\n",
    "\"The execution of token-based replay in pm4py permits to obtain detailed information about transitions that did not execute correctly, or activities that are in the log and not in the model. In particular, executions that do not match the model are expected to take longer throughput time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diagnosis(log, folder_prob):\n",
    "    unpredicted_activity_details = []\n",
    "    resource_count = {}  # Diccionario para contar la cantidad de veces que cada recurso realiza actividades no deseadas\n",
    "    resource_count_2={}\n",
    "    for trace in log:\n",
    "        # Acceder a los atributos en el nivel superior de la traza\n",
    "        attributes = trace.attributes\n",
    "        for event in trace:\n",
    "            activity_name = event['concept:name']\n",
    "            resource_t = event.get('org:resource', '')\n",
    "            if resource_t in resource_count_2:\n",
    "                resource_count_2[resource_t] += 1\n",
    "            else:\n",
    "                resource_count_2[resource_t] = 1\n",
    "            if activity_name not in [transition.label for transition in net_comp.transitions]:\n",
    "                # Obtener detalles adicionales del evento\n",
    "                case_id = attributes.get('concept:name', '')  # Obtener el valor de 'concept:name' o una cadena vacía si no existe\n",
    "                timestamp = event.get('time:timestamp', '')\n",
    "                resource = event.get('org:resource', '')\n",
    "                # Agregar detalles de la actividad no prevista al conjunto de datos\n",
    "                unpredicted_activity_details.append({\n",
    "                    \"ID case\": case_id,\n",
    "                    \"Activity\": activity_name,\n",
    "                    \"Time_Stamp\": timestamp,\n",
    "                    \"Resource\": resource,\n",
    "                    \"Event\": event  # Agregar el evento completo\n",
    "                })\n",
    "                \n",
    "                # Actualizar el contador de recursos para actividades no deseadas\n",
    "                if resource in resource_count:\n",
    "                    resource_count[resource] += 1\n",
    "                else:\n",
    "                    resource_count[resource] = 1\n",
    "\n",
    "    # Path to the output CSV file\n",
    "    csv_file_path = folder_prob + \"/unpredicted_activities.csv\"\n",
    "\n",
    "    # Create or open the CSV file in write mode\n",
    "    with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "        # Define the columns of the CSV file\n",
    "        fieldnames = [\"ID_case\", \"Activity\", \"Time_Stamp\", \"Resource\"]\n",
    "\n",
    "        # Create the CSV writer\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header row to the CSV file\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        # Write unpredicted activity details to the CSV file\n",
    "        for activity_details in unpredicted_activity_details:\n",
    "            csv_writer.writerow({\n",
    "                \"ID_case\": activity_details[\"ID case\"],\n",
    "                \"Activity\": activity_details[\"Activity\"],\n",
    "                \"Time_Stamp\": activity_details[\"Time_Stamp\"],\n",
    "                \"Resource\": activity_details[\"Resource\"]\n",
    "            })\n",
    "            csv_writer.writerow({})\n",
    "\n",
    "    print(f\"Unpredicted activity details have been saved to '{csv_file_path}'.\")\n",
    "\n",
    "    # Encuentra el recurso con la mayoría de actividades no deseadas\n",
    "    if  resource_count:\n",
    "        most_unwanted_resource = max(resource_count, key=resource_count.get)\n",
    "        count_of_unwanted_activities = resource_count[most_unwanted_resource]\n",
    "    if resource_count:\n",
    "    # Path para el archivo de texto\n",
    "        txt_file_path = folder_prob + \"/unwanted_resource_activities.txt\"\n",
    "\n",
    "        # Abrir el archivo de texto en modo escritura\n",
    "        with open(txt_file_path, mode='w') as txt_file:\n",
    "            for resource2, count2 in resource_count_2.items():\n",
    "                if resource2!=\"user\":\n",
    "                    txt_file.write(f\"Robot_{resource2}: {count2} actividades_totales\\n\")\n",
    "            # Recorrer el diccionario resource_count y escribir los detalles en el archivo\n",
    "            for resource, count in resource_count.items():\n",
    "                if resource!=\"user\":\n",
    "                    txt_file.write(f\"Robot_{resource}: {count} actividades_fallidas.\\n\")\n",
    "\n",
    "        print(f\"Detalles de recursos involucrados en actividades no deseadas han sido guardados en '{txt_file_path}'.\")\n",
    "    else:\n",
    "        print(\"No se encontraron actividades no deseadas en el registro.\")\n",
    "    if  resource_count:\n",
    "        print(f\"El recurso '{most_unwanted_resource}' realizó {count_of_unwanted_activities} actividades no deseadas.\")\n",
    "\n",
    "    actividades_totales = [0] * 6\n",
    "    actividades_fallidas = [0] * 6\n",
    "\n",
    "    with open(txt_file_path, 'r') as archivo:\n",
    "        print(txt_file_path)\n",
    "        # Lee cada línea del archivo\n",
    "        for linea in archivo:\n",
    "            # Divide la línea en dos partes en función del ':' como separador\n",
    "            etiqueta, valor = linea.strip().split(': ')\n",
    "            \n",
    "            # Extrae el número de actividades_totales o actividades_fallidas\n",
    "            numero = int(valor.split()[0])\n",
    "            \n",
    "            # Extrae el número de robot de la etiqueta\n",
    "            numero_robot = int(etiqueta.split('_')[1])\n",
    "            \n",
    "            # Comprueba el contenido de \"valor\" para determinar si es \"actividades_totales\" o \"actividades_fallidas\"\n",
    "            if \"actividades_totales\" in valor:\n",
    "                actividades_totales[numero_robot] = numero\n",
    "            elif \"actividades_fallidas\" in valor:\n",
    "                actividades_fallidas[numero_robot] = numero\n",
    "\n",
    "    # Imprime los resultados para verificar\n",
    "    print(\"actividades_totales:\", actividades_totales)\n",
    "    print(\"actividades_fallidas:\", actividades_fallidas)\n",
    "\n",
    "    # Datos de ejemplo para actividades y actividades fallidas\n",
    "    etiquetas = [\"r0\",\"r1\", \"r2\", \"r3\", \"r4\", \"r5\"]\n",
    "    ancho_barra = 0.35\n",
    "    # Coordenadas para las etiquetas en el eje x\n",
    "    x = range(len(etiquetas))\n",
    "    # Crear el gráfico de barras\n",
    "    plt.bar(x, actividades_totales, width=ancho_barra, label=\"Actividades Totales\", align=\"center\")\n",
    "    plt.bar([i + ancho_barra for i in x], actividades_fallidas, width=ancho_barra, label=\"Actividades Fallidas\", align=\"center\")\n",
    "    # Etiquetas en el eje x\n",
    "    plt.xticks([i + ancho_barra / 2 for i in x], etiquetas)\n",
    "    # Etiqueta del eje y\n",
    "    plt.ylabel(\"Cantidad\")\n",
    "    # Título del gráfico\n",
    "    plt.title(\"Actividades Totales y Actividades Fallidas realizadas por Robot\")\n",
    "    # Leyenda\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{folder_prob}/deteccion_robot.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for probability in probabilities:\n",
    "    plt.figure()\n",
    "    \n",
    "    file_path=f\"{new_folder}/prob_{probability}\"\n",
    "    if probability==0.25:\n",
    "        diagnosis(log_pl,file_path )\n",
    "    if probability==0.5:\n",
    "        diagnosis(log_pm,file_path )\n",
    "    if probability==0.75:\n",
    "        diagnosis(log_ph,file_path )\n",
    "\n",
    "file_path=f\"{new_folder}/prob_p_{prueba}\"\n",
    "diagnosis(log_prueba,file_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Throughput Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_case_durations = pm4py.get_all_case_durations(log_pi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycle Time and Waiting Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_log = interval_lifecycle.assign_lead_cycle_time(log_pl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribución de la duración de un caso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x es un conjunto de valores que representan la duración de los casos. Cada valor en x corresponde a un valor de duración de caso.\n",
    "\n",
    "y es un conjunto de valores que representan la densidad estimada para cada valor de duración de caso. En otras palabras, y muestra cuán densamente se distribuyen los casos en función de su duración. Un valor alto en y indica una mayor densidad de casos en ese intervalo de duración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pm4py.util import constants\n",
    "from pm4py.statistics.traces.generic.log import case_statistics\n",
    "x, y = case_statistics.get_kde_caseduration(log_pl, parameters={constants.PARAMETER_CONSTANT_TIMESTAMP_KEY: \"time:timestamp\"})\n",
    "\n",
    "from pm4py.visualization.graphs import visualizer as graphs_visualizer\n",
    "\n",
    "gviz = graphs_visualizer.apply_plot(x, y, variant=graphs_visualizer.Variants.CASES)\n",
    "graphs_visualizer.view(gviz)\n",
    "\n",
    "gviz = graphs_visualizer.apply_semilogx(x, y, variant=graphs_visualizer.Variants.CASES)\n",
    "graphs_visualizer.view(gviz)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of events over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "import numpy as np  # Importa NumPy\n",
    "\n",
    "x, y = attributes_filter.get_kde_date_attribute(log_pi, attribute=\"time:timestamp\")\n",
    "\n",
    "# Convierte x e y en arreglos de NumPy\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "from pm4py.visualization.graphs import visualizer as graphs_visualizer\n",
    "\n",
    "gviz = graphs_visualizer.apply_plot(x, y, variant=graphs_visualizer.Variants.DATES)\n",
    "graphs_visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some diagnosis to evaluate our model and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(file_write,form,fitness_str_t, prec_t,gen,simp):\n",
    "    with open(file_write, form) as file:\n",
    "        file.write(\"\\n\")\n",
    "        file.write(f\"Example:{new_name}\\n\")\n",
    "        file.write(f\"actions: {actions}\"+\"\\n\")\n",
    "        file.write(f\"probability:{probability}\\n\")\n",
    "        file.write(fitness_str_t+\"\\n\")\n",
    "        file.write(prec_t+\"\\n\")\n",
    "\n",
    "        file.write(gen+\"\\n\")\n",
    "        file.write(simp+\"\\n\")\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.conformance.alignments.decomposed import algorithm as decomp_alignments\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as rp_fitness_evaluator\n",
    "\n",
    "\n",
    "for probability in probabilities:\n",
    "    file_path=f\"{new_folder}/prob_{probability}\"\n",
    "    if probability==0.25:\n",
    "        log_dig=log_pl\n",
    "        folder_prob=f\"{dir_tutti}/Logs/model_evaluation_0.25.txt\"\n",
    "\n",
    "    if probability==0.5:\n",
    "        log_dig=log_pm\n",
    "        folder_prob=f\"{dir_tutti}/Logs/model_evaluation_0.5.txt\"\n",
    "\n",
    "    if probability==0.75:\n",
    "        log_dig=log_ph\n",
    "        folder_prob=f\"{dir_tutti}/Logs/model_evaluation_0.75.txt\"\n",
    "    if probability==1:\n",
    "        log_dig=log_pi\n",
    "        folder_prob=f\"{dir_tutti}/Logs/model_evaluation_1.txt\"\n",
    "    \n",
    "    fitness_t = pm4py.fitness_token_based_replay(log_dig, net_comp, im_comp, fm_comp)\n",
    "    # Initialize an empty string to build the dictionary representation\n",
    "    fitness_str_t = \"\"\n",
    "    # Build the string with spaces and line breaks between each element\n",
    "    for key, value in fitness_t.items():\n",
    "        fitness_str_t += f\"{key}: {value}\\n\"\n",
    "\n",
    "    prec_t = pm4py.precision_token_based_replay(log_dig, net_comp, im_comp, fm_comp)\n",
    "\n",
    "    prec_t = \"precision: \"+ str(prec_t)\n",
    "\n",
    "    gen = generalization_evaluator.apply(log_dig, net_comp, im_comp, fm_comp)\n",
    "    gen=\"generalization: \" + str(gen)\n",
    "\n",
    "    simp = simplicity_evaluator.apply(net_comp)\n",
    "    simp= \"simplicity: \" + str(simp)\n",
    "    # Save the string to a text file, overwriting the previous content\n",
    "    write(f\"{file_path}/model_evaluation.txt\", \"w\", fitness_str_t, prec_t,gen,simp)\n",
    "    write(f\"{file_path}/model_evaluation.txt\", \"w\", fitness_str_t, prec_t,gen,simp)\n",
    "    write(f\"{new_folder}/model_evaluation.txt\", \"a\", fitness_str_t, prec_t,gen,simp)\n",
    "    write(f\"{folder_prob}\", \"a\", fitness_str_t, prec_t,gen,simp)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
